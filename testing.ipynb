{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing\n",
    "\n",
    "## Why is Testing Important?\n",
    "\n",
    "Testing is a fundamental practice in software development. It allows us to:\n",
    "\n",
    "* Ensure code quality by catching bugs early and keeping them **out of production**\n",
    "\n",
    "* Regular testing helps maintain the reliability and stability of our software so that we can **confidently expand** our projects without time wasted finding and fixing regression\n",
    "\n",
    "* Having automated tests **support efforts to build onto our codebase** without fear of introducing new bugs \n",
    "\n",
    "* Tests help **new engineers onboard** without anxiety\n",
    "\n",
    "* Tests provide **documentation** of how our code is supposed to work for collaborators"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bb47eff3a262dce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Types of Tests\n",
    "\n",
    "There are many types of tests...\n",
    "\n",
    "* functional testing (i.e. testing the functionality)\n",
    "* non-functional testing (i.e. testing the system as a whole)\n",
    "\n",
    "* unit testing (atomic)\n",
    "* integration testing (interrelated functionality)\n",
    "* user acceptance testing (does it fulfill the ask)\n",
    "* end-to-end testing (full user/system workflows)\n",
    "\n",
    "* usability testing (human testers)\n",
    "* accessibility testing (human testers)\n",
    "* load testing (simulate regular traffic)\n",
    "* stress testing (simulate worst case traffic)\n",
    "* penetration testing (\"hackers\")\n",
    "* fuzz testing (random, weird inputs)\n",
    "* compliance testing (does it comply with prescribed regulations)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34a69328ea7acb4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test-Driven Development a.k.a. Fail Fast! \n",
    "\n",
    "Test-Driven Development (TDD) is an **iterative** software development approach that emphasizes **writing tests before writing the actual code**.\n",
    "\n",
    "Writing the test first makes you think about what the functionality really needs to do. You're writing a specification, otherwise referred to as a requirements document, before you write your \"real\" code.\n",
    "\n",
    "You will sometimes see the TDD cycle referred to as \"red-green-refactor\":\n",
    "\n",
    "1. [RED] Write a that defines the expected behavior and outcomes of the function\n",
    "2. [RED] Run the test... see that it fails because there's no functionality defined yet\n",
    "3. [GREEN] Write the minimum code to pass the test\n",
    "4. [GREEN] Run the test... see that it succeeds (if it fails... keep at it!)\n",
    "5. [REFACTOR] Check over your function and your test, make any tweaks to make the code better or more descriptive/self-documenting/faster\n",
    "6. [REFACTOR] Cycle between tweaking the function and running the test until the test passes and you are happy with the code\n",
    "\n",
    "Working in this way forces you to be intentional about your changes and to really think through what the inputs and outputs should be. It also prevents the inevitable headache of debugging an \"over-engineered\" monolith of code... sometimes for days... which is usually extremely demoralizing. \n",
    "\n",
    "The resulting code will be more modular and decoupled, making it more reusable. You'll find your code being used by other engineers because your test-covered, self-documenting functions are consistently reliable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "307ad7c221d26da8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fundamentals of Testing in Python\n",
    "\n",
    "### [pytest](https://realpython.com/pytest-python-testing/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "863d4887a3234c89"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (866755502.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[1], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    git clone\u001B[0m\n\u001B[0m        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git clone \n",
    "\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "python -m pip install pytest"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T19:03:00.526096Z",
     "start_time": "2024-06-02T19:03:00.522455Z"
    }
   },
   "id": "cc5b253ff18d75a3",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TDD in Action!\n",
    "\n",
    "We're going to write some utilities for working with grades. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a6908ba43b63307"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mkdir gradebook"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d385c5214e7a2ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "But we're writing tests before writing the actual functions, so..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5299562b5e1ee266"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mkdir tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e19185a440085ab3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have to remember a few things as we start to write unit tests.\n",
    "\n",
    "* Unit tests are designed to **test individual bits of logic** (\"units\") in isolation, with no dependencies (more on that later)\n",
    "* Each test should focus on a single behavior or aspect of the code, i.e. don't try to test every possible outcome at once\n",
    "\n",
    "We're going to follow the **arrange -> act -> assert** pattern:\n",
    "    - set up the conditions for your test (arrange)\n",
    "    - execute the unit of code being tested (act)\n",
    "    - check that the result is what was expected (assert)\n",
    "\n",
    "Lets write a test..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df89e32a890476cc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test/unit_test.py\n",
    "\n",
    "from gradebook import average_grade\n",
    "\n",
    "def test():\n",
    "    grades = [90, 80, 70] # arrange \n",
    "    average = average_grade(grades) # act\n",
    "    assert average == 80 # assert"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a83dafe379c9cde2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test names should always be descriptive, so lets give it a better name for posterity... and trim it down to be more Python-y now that you get the arrange-act-assert point."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83708efa54d164a4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test/unit_test.py\n",
    "\n",
    "from gradebook import average_grade\n",
    "\n",
    "def average_grade_returns_average_of_grades_provided():\n",
    "    assert average_grade([90, 80, 70]) == 80"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61bb24ac5223439c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Does this cover every condition we may want to test for? What if the list is empty?\n",
    "\n",
    "@TODO empty list result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "322d9c6055c7b6d6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test/unit_test.py\n",
    "\n",
    "from gradebook import average_grade\n",
    "\n",
    "def average_grade_returns_zero_if_no_grades_provided():\n",
    "    assert average_grade([]) == 0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1585d24b3c3ee386"
  },
  {
   "cell_type": "markdown",
   "source": [
    "One might argue that the average of an empty list is actually nothing... sending a clear message that the list of grades was empty and not a set of zeros, but gathering requirements is a topic for another day. Writing the test first helps to shed light on ambiguity."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d18eacfc7aa09188"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test/unit_test.py\n",
    "\n",
    "from gradebook import average_grade\n",
    "\n",
    "def average_grade_returns_zero_if_no_grades_provided():\n",
    "    assert average_grade([]) is None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac3d2922793b82e7"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e03b94dc7fc136a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# gradebook/grades.py\n",
    "\n",
    "\n",
    "def calculate_average(grades):\n",
    "    \"\"\"compute the average of the grades\"\"\"\n",
    "    if not grades:\n",
    "        return 0\n",
    "    return sum(grades) / len(grades)\n",
    "\n",
    "\n",
    "def find_highest_grade(grades):\n",
    "    \"\"\"find the highest grade\"\"\"\n",
    "    if not grades:\n",
    "        return None\n",
    "    return max(grades)\n",
    "\n",
    "\n",
    "def find_lowest_grade(grades):\n",
    "    \"\"\"find the lowest grade\"\"\"\n",
    "    if not grades:\n",
    "        return None\n",
    "    return min(grades)\n",
    "\n",
    "\n",
    "def determine_letter_grade(grade):\n",
    "    \"\"\"return the letter grade for a numeric grade\"\"\"\n",
    "    if grade >= 90:\n",
    "        return \"A\"\n",
    "    elif grade >= 80:\n",
    "        return \"B\"\n",
    "    elif grade >= 70:\n",
    "        return \"C\"\n",
    "    elif grade >= 60:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"F\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "776c2e7c718c71e0"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1732189433a1cdfb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pytest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "197bd456c71d3a29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output then indicates the status of each test using a syntax similar to unittest:\n",
    "\n",
    "A dot (.) means that the test passed.\n",
    "An F means that the test has failed.\n",
    "An E means that the test raised an unexpected exception."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cf6a81fd549ce57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "mention monkeypatch\n",
    "\n",
    "avoid tightly coupled code\n",
    "\n",
    "use descriptive test names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Use mocking frameworks to simulate complex dependencies and isolate the unit of code under test\n",
    "monkeypatch\n",
    "\n",
    "Tests should not rely on each other. Each test should be able to run independently in any order.\n",
    "\n",
    "address Edge Cases\n",
    "Test for a range of inputs, including edge cases and boundary conditions\n",
    "\n",
    "Unit tests should run quickly to integrate well into CI pipelines\n",
    "pytest\n",
    "\n",
    "Use setup methods or fixtures to prepare test data (no hardcoding)\n",
    "\n",
    "Refactor tests as you refactor code\n",
    "\n",
    "Run Tests Frequently\n",
    "\n",
    "Aim for high code coverage, but don't obsess\n",
    "\n",
    "Statement coverage measures how many statements in the code were executed during testing\n",
    "branch coverage measures how many branches in the code were executed\n",
    "Path coverage is the most comprehensive metric and measures how many unique paths through the code were executed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Provide comments or documentation to explain the purpose of complex tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "519a2d58faccc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [Code Coverage](https://martinxpn.medium.com/test-coverage-in-python-with-pytest-86-100-days-of-python-a3205c77296)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dce976c73070cfb0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pip install pytest-cov\n",
    "pytest --cov=gradebook"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8a00635d9e81f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stmts: The total number of statements in the package.\n",
    "Miss: The number of statements that were not executed during testing.\n",
    "Cover: The percentage of statements that were executed during testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4c4df4ab7bcdbc9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
