{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing\n",
    "\n",
    "## Why is Testing Important?\n",
    "\n",
    "Testing is a fundamental practice in software development. It allows us to:\n",
    "\n",
    "* Ensure code quality by catching bugs early and keeping them **out of production**\n",
    "\n",
    "* Regular testing helps maintain the reliability and stability of our software so that we can **confidently expand** our projects without time wasted finding and fixing regression\n",
    "\n",
    "* Having automated tests **support efforts to build onto our codebase** without fear of introducing new bugs \n",
    "\n",
    "* Tests help **new engineers onboard** without anxiety\n",
    "\n",
    "* Tests provide **documentation** of how our code is supposed to work for collaborators"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bb47eff3a262dce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Types of Tests\n",
    "\n",
    "There are many types of tests...\n",
    "\n",
    "* functional testing (i.e. testing the functionality)\n",
    "* non-functional testing (i.e. testing the system as a whole)\n",
    "\n",
    "* unit testing (atomic)\n",
    "* integration testing (interrelated functionality)\n",
    "* user acceptance testing (does it fulfill the ask)\n",
    "* end-to-end testing (full user/system workflows)\n",
    "\n",
    "* usability testing (human testers)\n",
    "* accessibility testing (human testers)\n",
    "* load testing (simulate regular traffic)\n",
    "* stress testing (simulate worst case traffic)\n",
    "* penetration testing (\"hackers\")\n",
    "* fuzz testing (random, weird inputs)\n",
    "* compliance testing (does it comply with prescribed regulations)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34a69328ea7acb4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test-Driven Development a.k.a. Fail Fast! \n",
    "\n",
    "Test-Driven Development (TDD) is an **iterative** software development approach that emphasizes **writing tests before writing the actual code**.\n",
    "\n",
    "Writing the test first makes you think about what the functionality really needs to do. You're writing a specification, otherwise referred to as a requirements document, before you write your \"real\" code.\n",
    "\n",
    "You will sometimes see the TDD cycle referred to as \"red-green-refactor\":\n",
    "\n",
    "1. [RED] Write a that defines the expected behavior and outcomes of the function\n",
    "2. [RED] Run the test... see that it fails because there's no functionality defined yet\n",
    "3. [GREEN] Write the minimum code to pass the test\n",
    "4. [GREEN] Run the test... see that it succeeds (if it fails... keep at it!)\n",
    "5. [REFACTOR] Check over your function and your test, make any tweaks to make the code better or more descriptive/self-documenting/faster\n",
    "6. [REFACTOR] Cycle between tweaking the function and running the test until the test passes and you are happy with the code\n",
    "\n",
    "Working in this way forces you to be intentional about your changes and to really think through what the inputs and outputs should be. It also prevents the inevitable headache of debugging an \"over-engineered\" monolith of code... sometimes for days... which is usually extremely demoralizing. \n",
    "\n",
    "The resulting code will be more modular and decoupled, making it more reusable. You'll find your code being used by other engineers because your test-covered, self-documenting functions are consistently reliable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "307ad7c221d26da8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fundamentals of Testing in Python\n",
    "\n",
    "### [pytest](https://realpython.com/pytest-python-testing/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "863d4887a3234c89"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (866755502.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[1], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    git clone\u001B[0m\n\u001B[0m        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git clone \n",
    "\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "python -m pip install pytest"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T19:03:00.526096Z",
     "start_time": "2024-06-02T19:03:00.522455Z"
    }
   },
   "id": "cc5b253ff18d75a3",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Developing Unit Tests\n",
    "\n",
    "We're going to write some utility functions for working with number grades. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a6908ba43b63307"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mkdir gradebook"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d385c5214e7a2ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "But we're writing tests before writing the actual functions, so..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5299562b5e1ee266"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mkdir tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e19185a440085ab3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have to remember a few things as we start to write unit tests.\n",
    "\n",
    "* Unit tests are designed to **test individual bits of logic** (\"units\") in isolation, with no dependencies (more on that later)\n",
    "* Each test should focus on a single behavior or aspect of the code, i.e. don't try to test every possible outcome at once\n",
    "\n",
    "We're going to follow the **arrange -> act -> assert** pattern:\n",
    "    - set up the conditions for your test (arrange)\n",
    "    - execute the unit of code being tested (act)\n",
    "    - check that the result is what was expected (assert)\n",
    "\n",
    "Lets write a test..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df89e32a890476cc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test/unit_test.py\n",
    "\n",
    "import pytest\n",
    "from gradebook.grades import calculate_average\n",
    "\n",
    "def test():\n",
    "    grades = [90, 80, 70] # arrange \n",
    "    average = calculate_average(grades) # act\n",
    "    assert average == 80 # assert"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a83dafe379c9cde2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "run it..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e1e9d4767a25046"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pytest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "115358b3cfe43f8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test names should always be descriptive, so lets give it a better name for posterity... and trim it down to be more Python-y now that you get the arrange-act-assert point."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83708efa54d164a4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test/unit_test.py\n",
    "\n",
    "import pytest\n",
    "from gradebook.grades import calculate_average\n",
    "\n",
    "def average_grade_returns_average_of_grades_provided(): # extra descriptive!\n",
    "    assert calculate_average([90, 80, 70]) == 80"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61bb24ac5223439c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Technically, we should not be hardcoding any values. We should use fixtures instead."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cd9afcabc30f653"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#@ TODO fixture code and execution"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca53376ce1b04b02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Does this test cover every situation we may want to test for? What if the list is empty?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "322d9c6055c7b6d6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test/unit_test.py\n",
    "\n",
    "import pytest\n",
    "from gradebook.grades import calculate_average\n",
    "\n",
    "def average_grade_returns_zero_if_no_grades_provided(): # appropriately descriptive\n",
    "    assert calculate_average([]) == 0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4259fa8a0a6de7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "One might argue that the average of an empty list is actually nothing... sending a clear message that the list of grades was empty and not a set of zeros, but gathering requirements is a topic for another day. Writing the test first helps to shed light on ambiguity."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d18eacfc7aa09188"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test/unit_test.py\n",
    "\n",
    "import pytest\n",
    "from gradebook.grades import calculate_average\n",
    "\n",
    "def average_grade_returns_zero_if_no_grades_provided():\n",
    "    assert calculate_average([]) is None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac3d2922793b82e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a large codebase, I have seen automated tests take thirty minutes to run because there are so many, each atomic, each \"arranging\" its setup. It may be beneficial to start using [markers](https://pytest-with-eric.com/pytest-best-practices/pytest-markers/) straight out of the gate so you can run only the tests in the scope you are modifying. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1732189433a1cdfb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test/unit_test.py\n",
    "\n",
    "import pytest\n",
    "from gradebook.grades import calculate_average\n",
    "\n",
    "@pytest.mark.test\n",
    "def average_grade_returns_zero_if_no_grades_provided():\n",
    "    assert calculate_average([]) is None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9e01c2f527e81ec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pytest -m test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "197bd456c71d3a29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Incidentally, we need to register markers in pyproject.toml to prevent warnings.\n",
    "\n",
    "```yaml\n",
    "[tool.pytest.ini_options]\n",
    "markers = [\n",
    "    \"test: my first custom mark\",\n",
    "]\n",
    "```\n",
    "\n",
    "Then when we run `pytest --markers`, we will see our custom mark at the top of the list!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24c6fc2f4093122c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pytest --markers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "756d26261ecdda51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lots of marker settings available, including conditional skipping and intentional failure (as a placeholder to address an issue) https://pytest-with-eric.com/pytest-best-practices/pytest-markers/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f561b41c6079f2c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we add more tests, some additional considerations...\n",
    "\n",
    "* Tests should not rely on each other. \n",
    "* Each test should be able to run independently in any order.\n",
    "* Run tests frequently, preferably automatically as a pre-commit hook (which can't really be enforced) or, much better, as part of a CI workflow.\n",
    "* Unit tests should run quickly to integrate well into CI pipelines."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cf6a81fd549ce57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets add some more tests and some more functions and then rerun our new test suite."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd744bc7ee9f0c82"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pytest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fdf97cf1fcf7605"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output then indicates the status of each test:\n",
    "\n",
    "A dot (.) means that the test passed\n",
    "An F means that the test has failed\n",
    "An E means that the test raised an unexpected exception"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9c7a6be7bbf3582"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hey look, integration tests!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "166debb8921957d4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6ab32cfdedf2b9e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mocking Complex Dependencies\n",
    "\n",
    "Use a mocking framework to simulate complex dependencies and isolate the unit of code under test e.g. [pytest-mock] (https://pytest-with-eric.com/mocking/pytest-mocking/)\n",
    "\n",
    "You can mock pretty much anything:\n",
    "- REST API requests and responses\n",
    "- Database queries\n",
    "- Built-in functions and constants\n",
    "- Complex classes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaee3243dd5bd362"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pip install pytest-mock\n",
    "# @ TODO run unit_test_write_grades"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e54134c294d53e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [Code Coverage](https://martinxpn.medium.com/test-coverage-in-python-with-pytest-86-100-days-of-python-a3205c77296)\n",
    "\n",
    "Aim for high code coverage, but don't obsess - it will never be 100%.\n",
    "\n",
    "Statement coverage measures how many statements in the code were executed during testing\n",
    "branch coverage measures how many branches in the code were executed\n",
    "Path coverage is the most comprehensive metric and measures how many unique paths through the code were executed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "519a2d58faccc1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pip install pytest-cov\n",
    "pytest --cov=gradebook"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8a00635d9e81f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stmts: The total number of statements in the package.\n",
    "Miss: The number of statements that were not executed during testing.\n",
    "Cover: The percentage of statements that were executed during testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4c4df4ab7bcdbc9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
