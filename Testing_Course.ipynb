{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Testing: What, Why and How To"
      ],
      "metadata": {
        "collapsed": false,
        "id": "3bb47eff3a262dce"
      },
      "id": "3bb47eff3a262dce"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Many Types of [Software] Tests\n",
        "\n",
        "There are many types of tests broadly divided into two categories:\n",
        "\n",
        "* **functional testing** ‚¨ú üìú testing the functionality\n",
        "* **non-functional testing** üí£ üî® testing the system as a whole without regard for intended functionality\n",
        "\n",
        "Another dimension you can categorize tests by is whether they are:\n",
        "* **automated** ü§ñ\n",
        "* or **manual** üßë\n",
        "\n",
        "The specific categories of tests you will most likely see out in the wild include:\n",
        "\n",
        "* **unit** testing (atomic functionality)\n",
        "* **integration** testing (interrelated functionality)\n",
        "* **user acceptance** testing (does it fulfill the user contract)\n",
        "* **compliance** testing (does it comply with prescribed regulations)\n",
        "* **end-to-end** testing (full user/system workflows)\n",
        "* **usability** testing (human friendly)\n",
        "* **accessibility** testing (disability friendly)\n",
        "* **load** testing (regular traffic)\n",
        "* **stress** testing (worst case traffic)\n",
        "* **penetration** testing (hackers)\n",
        "* **fuzz** testing (random inputs)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "34a69328ea7acb4e"
      },
      "id": "34a69328ea7acb4e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We will be focusing on functional, automated, **unit tests**."
      ],
      "metadata": {
        "id": "oOsw2ccf8n6t"
      },
      "id": "oOsw2ccf8n6t"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why should we take the time to write automated tests?\n",
        "\n",
        "Testing is a fundamental practice in software development. It allows us to:\n",
        "\n",
        "* Tests provide **documentation** üìú of how our code is supposed to work for collaborators and for our future selves\n",
        "\n",
        "* Tests help **engineers onboard into established codebases** without anxiety üò¨ üôà\n",
        "\n",
        "* Regular testing helps maintain the reliability and stability of our codebase so that we can **confidently expand** our projects without time wasted tracking down regressions\n",
        "\n",
        "* Ensure code quality by **catching bugs üêõ early** and keeping them **out of production** üö®"
      ],
      "metadata": {
        "id": "nDynVot36kMs"
      },
      "id": "nDynVot36kMs"
    },
    {
      "cell_type": "markdown",
      "source": [
        " There are **many languages** out there. We have to choose one, so we'll use Python here. The **high level concepts and features of our testing framework** are **fairly universal**."
      ],
      "metadata": {
        "id": "3UG2YSpyV0Yv"
      },
      "id": "3UG2YSpyV0Yv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The pytest Testing Framework\n",
        "\n",
        "`unittest` is baked in to Python, but we'll be using [pytest](https://realpython.com/pytest-python-testing/)  \n",
        "- less setup\n",
        "- simple syntax\n",
        "- large ecosystem of [plugins](https://pytest.org/en/stable/reference/plugin_list.html)\n",
        "\n",
        "- auto-finds tests, just as long as you make sure your test files and functions start with `test_`!"
      ],
      "metadata": {
        "id": "5m9YOkdepjra"
      },
      "id": "5m9YOkdepjra"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test-Driven Development\n",
        "\n",
        "Test-Driven Development (TDD) is an iterative software development approach that encourages writing tests before writing the actual code.\n",
        "\n",
        "### Fail Fast üí£ üöÄ\n",
        "\n",
        "Writing the test first makes you stop to **carefully consider** what a function really needs to do. You're writing a specification, otherwise referred to as a requirements document, before you write your \"real\" code.\n",
        "\n",
        "You will sometimes see the TDD cycle referred to as:\n",
        "\n",
        "### üî¥ üü¢ üîÑ Red Green Refactor\n",
        "\n",
        "1. üî¥ **Write** a test that defines the expected behavior and outcomes of the function\n",
        "2. üî¥ **Run** the test and see that it fails because there's no functionality defined yet\n",
        "3. üü¢ **Write** the minimum code to pass the test\n",
        "4. üü¢ **Run** the test... see that it succeeds (if it fails... keep at it!)\n",
        "5. üîÑ **Check** over your function and your test, make any **tweaks** to make the code better or more descriptive/self-documenting/faster\n",
        "\n",
        "**Cycle** between tweaking the function and running the test until the test passes and you are happy with the functionality and the test code.\n",
        "\n",
        "Working in this way forces you to be **intentional** about the code you write and to really think through what the inputs and outputs should be. It also prevents the inevitable headache of debugging an **\"over-engineered\" monolith** of code... which is extremely demoralizing.\n",
        "\n",
        "The resulting code will be more **modular** and decoupled, making it more **reusable**. You'll find your code being used by other engineers because your test-covered, self-documenting functions are consistently reliable."
      ],
      "metadata": {
        "collapsed": false,
        "id": "307ad7c221d26da8"
      },
      "id": "307ad7c221d26da8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Unit Test\n",
        "\n",
        "We're going to write some utility functions for working with grades. First lets set up our environment."
      ],
      "metadata": {
        "collapsed": false,
        "id": "1a6908ba43b63307"
      },
      "id": "1a6908ba43b63307"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  python3-pip-whl python3-setuptools-whl\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip-whl python3-setuptools-whl python3.10-venv\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 2,473 kB of archives.\n",
            "After this operation, 2,884 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip-whl all 22.0.2+dfsg-1ubuntu0.4 [1,680 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-setuptools-whl all 59.6.0-1.2ubuntu0.22.04.1 [788 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3.10-venv amd64 3.10.12-1~22.04.3 [5,716 B]\n",
            "Fetched 2,473 kB in 2s (1,097 kB/s)\n",
            "Selecting previously unselected package python3-pip-whl.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 121918 files and directories currently installed.)\r\n",
            "Preparing to unpack .../python3-pip-whl_22.0.2+dfsg-1ubuntu0.4_all.deb ...\r\n",
            "Unpacking python3-pip-whl (22.0.2+dfsg-1ubuntu0.4) ...\r\n",
            "Selecting previously unselected package python3-setuptools-whl.\r\n",
            "Preparing to unpack .../python3-setuptools-whl_59.6.0-1.2ubuntu0.22.04.1_all.deb ...\r\n",
            "Unpacking python3-setuptools-whl (59.6.0-1.2ubuntu0.22.04.1) ...\r\n",
            "Selecting previously unselected package python3.10-venv.\r\n",
            "Preparing to unpack .../python3.10-venv_3.10.12-1~22.04.3_amd64.deb ...\r\n",
            "Unpacking python3.10-venv (3.10.12-1~22.04.3) ...\r\n",
            "Setting up python3-setuptools-whl (59.6.0-1.2ubuntu0.22.04.1) ...\r\n",
            "Setting up python3-pip-whl (22.0.2+dfsg-1ubuntu0.4) ...\r\n",
            "Setting up python3.10-venv (3.10.12-1~22.04.3) ...\r\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# set up code directory\n",
        "mkdir -p gradebook\n",
        "\n",
        "# colab will complain later without this\n",
        "apt install python3.10-venv"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-02T19:03:00.526096Z",
          "start_time": "2024-06-02T19:03:00.522455Z"
        },
        "id": "cc5b253ff18d75a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f950ab7-7f63-4dda-a9ab-ddeb32f3fcc8"
      },
      "id": "cc5b253ff18d75a3",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# set up python\n",
        "!python3 -m venv venv\n",
        "!source venv/bin/activate\n",
        "!python3 -m pip install pytest"
      ],
      "metadata": {
        "id": "gDfhxjerYIOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4078ae2-0dbd-47a9-d6fd-de599cbd2e81"
      },
      "id": "gDfhxjerYIOg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.4.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (24.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.2.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to remember a few things as we start to write unit tests.\n",
        "\n",
        "* Unit tests are designed to **test individual bits of logic** (\"units\"), self-contained, **no dependencies**\n",
        "* Each test should focus on a single behavior or outcome, i.e. **don't try to cover every possible scenario or outcome in one test**"
      ],
      "metadata": {
        "collapsed": false,
        "id": "df89e32a890476cc"
      },
      "id": "df89e32a890476cc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should approach building unit tests keeping in mind 3 steps:\n",
        "\n",
        "\n",
        "#### 1. Arrange\n",
        "set up the conditions for your test\n",
        "\n",
        "#### 2. Act\n",
        "invoke the unit of code being tested\n",
        "\n",
        "#### 3. Assert\n",
        "check that the result is what was expected"
      ],
      "metadata": {
        "id": "mLtDFSbJLUgG"
      },
      "id": "mLtDFSbJLUgG"
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir test"
      ],
      "metadata": {
        "id": "rAp5E3DBPsT-"
      },
      "id": "rAp5E3DBPsT-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test/test_averaging.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test/test_averaging.py\n",
        "import pytest\n",
        "from gradebook.grade_utils import calculate_average\n",
        "\n",
        "def test_averaging():\n",
        "    grades = [90, 80, 70] # arrange\n",
        "    average = calculate_average(grades) # act\n",
        "    assert average == 80 # assert"
      ],
      "metadata": {
        "id": "a83dafe379c9cde2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8ea5b8-856b-4429-8113-a85ce411a47a"
      },
      "id": "a83dafe379c9cde2",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets run `pytest`. Remember that we're **anticipating failure.**"
      ],
      "metadata": {
        "collapsed": false,
        "id": "7e1e9d4767a25046"
      },
      "id": "7e1e9d4767a25046"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 0 items / 1 error                                                                        \u001b[0m\n",
            "\n",
            "============================================== ERRORS ==============================================\n",
            "\u001b[31m\u001b[1m_____________________________ ERROR collecting test/test_averaging.py ______________________________\u001b[0m\n",
            "\u001b[31mImportError while importing test module '/content/test/test_averaging.py'.\n",
            "Hint: make sure your test modules/packages have valid Python names.\n",
            "Traceback:\n",
            "/usr/lib/python3.10/importlib/__init__.py:126: in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "test/test_averaging.py:2: in <module>\n",
            "    from gradebook.grade_utils import calculate_average\n",
            "E   ModuleNotFoundError: No module named 'gradebook.grade_utils'\u001b[0m\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mERROR\u001b[0m test/test_averaging.py\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "\u001b[31m========================================= \u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[31m =========================================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest"
      ],
      "metadata": {
        "id": "115358b3cfe43f8b",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6080009-572c-46bd-f411-e1b6e667a7d1"
      },
      "id": "115358b3cfe43f8b",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### That looks about right.\n",
        "\n",
        "Lets add a basic implementation."
      ],
      "metadata": {
        "id": "22OS2PnOb4TJ"
      },
      "id": "22OS2PnOb4TJ"
    },
    {
      "cell_type": "code",
      "source": [
        "!touch gradebook/__init__.py"
      ],
      "metadata": {
        "id": "xfKV8QpEWgeS"
      },
      "id": "xfKV8QpEWgeS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gradebook/grade_utils.py\n",
        "\n",
        "def calculate_average(grades):\n",
        "    return sum(grades) / len(grades)\n"
      ],
      "metadata": {
        "id": "2MDKa-HbI6ov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f821426-80c2-43b5-f827-bc96466182a0"
      },
      "id": "2MDKa-HbI6ov",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gradebook/grade_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-run our test..."
      ],
      "metadata": {
        "id": "AWfaS7-TJzK8"
      },
      "id": "AWfaS7-TJzK8"
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest"
      ],
      "metadata": {
        "id": "cfKIMTBJJ4BL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d91571-a3b7-4362-82da-b75960232852"
      },
      "id": "cfKIMTBJJ4BL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                                                   \u001b[0m\n",
            "\n",
            "test/test_averaging.py \u001b[32m.\u001b[0m\u001b[32m                                                                     [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I do üíö love üíö seeing that <span style='color:green'>green</span>."
      ],
      "metadata": {
        "id": "3B6VN3yXKAbx"
      },
      "id": "3B6VN3yXKAbx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to see which specific tests were executed, you can pass the `--verbose` or `-v` parameter to pytest."
      ],
      "metadata": {
        "id": "5gHmSlVybr-n"
      },
      "id": "5gHmSlVybr-n"
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest -v"
      ],
      "metadata": {
        "id": "lMKVtfynb1A-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6342a351-1a03-42eb-c8e2-a45bf9f22e45"
      },
      "id": "lMKVtfynb1A-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                                                   \u001b[0m\n",
            "\n",
            "test/test_averaging.py::test_averaging \u001b[32mPASSED\u001b[0m\u001b[32m                                                [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test names should always be **descriptive**, so lets give it a better name for those who come after us, and trim it down to be more Python-y now that you get the **arrange-act-assert** point."
      ],
      "metadata": {
        "collapsed": false,
        "id": "83708efa54d164a4"
      },
      "id": "83708efa54d164a4"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test/test_averaging.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test/test_averaging.py\n",
        "import pytest\n",
        "from gradebook.grade_utils import calculate_average\n",
        "\n",
        "def test_that_average_grade_returns_average_of_grades_provided():\n",
        "    assert calculate_average([90, 80, 70]) == 80\n"
      ],
      "metadata": {
        "id": "61bb24ac5223439c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b984fee8-93ee-4619-d8b9-d69355c679a6"
      },
      "id": "61bb24ac5223439c",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest -v"
      ],
      "metadata": {
        "id": "JjtBhtqTLZlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "399063d1-3652-4481-9760-0d176f103a38"
      },
      "id": "JjtBhtqTLZlR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                                                   \u001b[0m\n",
            "\n",
            "test/test_averaging.py::test_that_average_grade_returns_average_of_grades_provided \u001b[32mPASSED\u001b[0m\u001b[32m    [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fixtures\n",
        "Technically, we should not be hardcoding any values. We should use fixtures instead.\n",
        "\n",
        "This helps by **documenting** the expected values for anyone who looks at this code and also facilitates their **adding more tests** by giving them building blocks to start with."
      ],
      "metadata": {
        "collapsed": false,
        "id": "1cd9afcabc30f653"
      },
      "id": "1cd9afcabc30f653"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test/test_averaging.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test/test_averaging.py\n",
        "import pytest\n",
        "from gradebook.grade_utils import calculate_average\n",
        "\n",
        "@pytest.fixture\n",
        "def some_grades():\n",
        "    return [90, 80, 70]\n",
        "\n",
        "def test_that_average_grade_returns_average_of_grades_provided(some_grades):\n",
        "    assert calculate_average(some_grades) == 80"
      ],
      "metadata": {
        "id": "ca53376ce1b04b02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d286d4e3-c63d-4ed5-f8c8-c63ac663beaf"
      },
      "id": "ca53376ce1b04b02",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest"
      ],
      "metadata": {
        "id": "MptAHDz0QUUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1abdf1f6-b75a-4191-bfff-2b9aa17913a0"
      },
      "id": "MptAHDz0QUUL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                                                   \u001b[0m\n",
            "\n",
            "test/test_averaging.py \u001b[32m.\u001b[0m\u001b[32m                                                                     [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maybe** we should discuss that `pytest` **abbreviated output**.\n",
        "\n",
        "- A green dot `.` means that the test passed\n",
        "- An `F` means that the test has failed\n",
        "- An `E` means that the test raised an unexpected exception"
      ],
      "metadata": {
        "id": "DWN0uNw8mnZa"
      },
      "id": "DWN0uNw8mnZa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets step back and consider whether this test covers every situation we may want to test for. ü§î\n",
        "\n",
        "What if the list is **empty**?"
      ],
      "metadata": {
        "collapsed": false,
        "id": "322d9c6055c7b6d6"
      },
      "id": "322d9c6055c7b6d6"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test/test_averaging_some_more.py\n",
        "import pytest\n",
        "from gradebook.grade_utils import calculate_average\n",
        "\n",
        "@pytest.fixture\n",
        "def no_grades():\n",
        "    return []\n",
        "\n",
        "def test_that_average_of_no_grades_still_works(no_grades):\n",
        "    assert calculate_average(no_grades) is None"
      ],
      "metadata": {
        "id": "7iLsIgb5V4U_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff782a02-e8a0-4ea1-849f-7cd2bc23a5b0"
      },
      "id": "7iLsIgb5V4U_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test/test_averaging_some_more.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -v"
      ],
      "metadata": {
        "id": "1wSzOwW5Yu_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af365b04-d5f4-4e9e-d9fc-1256a67b8690"
      },
      "id": "1wSzOwW5Yu_9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                                                  \u001b[0m\n",
            "\n",
            "test/test_averaging.py::test_that_average_grade_returns_average_of_grades_provided \u001b[32mPASSED\u001b[0m\u001b[32m    [ 50%]\u001b[0m\n",
            "test/test_averaging_some_more.py::test_that_average_of_no_grades_still_works \u001b[31mFAILED\u001b[0m\u001b[31m          [100%]\u001b[0m\n",
            "\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m____________________________ test_that_average_of_no_grades_still_works ____________________________\u001b[0m\n",
            "\n",
            "no_grades = []\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_that_average_of_no_grades_still_works\u001b[39;49;00m(no_grades):\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m calculate_average(no_grades) \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtest/test_averaging_some_more.py\u001b[0m:9: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "grades = []\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcalculate_average\u001b[39;49;00m(grades):\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96msum\u001b[39;49;00m(grades) / \u001b[96mlen\u001b[39;49;00m(grades)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       ZeroDivisionError: division by zero\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mgradebook/grade_utils.py\u001b[0m:3: ZeroDivisionError\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m test/test_averaging_some_more.py::\u001b[1mtest_that_average_of_no_grades_still_works\u001b[0m - ZeroDivisionError: division by zero\n",
            "\u001b[31m=================================== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[31m ====================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Okay, now, lets **refactor** our function to handle the empty list."
      ],
      "metadata": {
        "id": "Ek_rQDo5Y1md"
      },
      "id": "Ek_rQDo5Y1md"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gradebook/grade_utils.py\n",
        "\n",
        "def calculate_average(grades):\n",
        "    if not grades:\n",
        "      return None\n",
        "    return sum(grades) / len(grades)"
      ],
      "metadata": {
        "id": "DGpd1EigY08v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787a1a1e-165f-46df-f577-1b77b909b566"
      },
      "id": "DGpd1EigY08v",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting gradebook/grade_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I maintain that the average of an empty list is nothing, sending a clear message that the list of grades was empty and not a set of zeros, which could be a valid input.\n",
        "\n",
        "Gathering requirements is a topic for another day, but just wanted to point out how **writing tests first helps to shed light on ambiguity** so you can have a chat with the user requesting the change or the \"product owner\" in a larger org."
      ],
      "metadata": {
        "collapsed": false,
        "id": "d18eacfc7aa09188"
      },
      "id": "d18eacfc7aa09188"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest"
      ],
      "metadata": {
        "id": "34xqDVtVWFxa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3a18cf2-356f-432d-cf6a-468c01550f95"
      },
      "id": "34xqDVtVWFxa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                                                  \u001b[0m\n",
            "\n",
            "test/test_averaging.py \u001b[32m.\u001b[0m\u001b[32m                                                                     [ 50%]\u001b[0m\n",
            "test/test_averaging_some_more.py \u001b[32m.\u001b[0m\u001b[32m                                                           [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, you know, erring on the side of caution isn't a bad thing. It doesn't hurt to introduce tests just to make sure things work how you expect, e.g. if the input is not an empty list but nothing:"
      ],
      "metadata": {
        "id": "0qF5gjQ2NkPR"
      },
      "id": "0qF5gjQ2NkPR"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test/test_averaging_yet_again.py\n",
        "import pytest\n",
        "from gradebook.grade_utils import calculate_average\n",
        "\n",
        "def test_that_average_of_nothing_still_does_what_is_expected():\n",
        "    assert calculate_average(None) is None"
      ],
      "metadata": {
        "id": "t3q7yjyIbsLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d53afb-5e3b-4059-f52b-9ae684b571db"
      },
      "id": "t3q7yjyIbsLv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test/test_averaging_yet_again.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -v"
      ],
      "metadata": {
        "id": "PbJ6_u3-cEvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c50515e3-4c0a-4185-ba8c-47b36584b8c2"
      },
      "id": "PbJ6_u3-cEvu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items                                                                                  \u001b[0m\n",
            "\n",
            "test/test_averaging.py::test_that_average_grade_returns_average_of_grades_provided \u001b[32mPASSED\u001b[0m\u001b[32m    [ 33%]\u001b[0m\n",
            "test/test_averaging_some_more.py::test_that_average_of_no_grades_still_works \u001b[32mPASSED\u001b[0m\u001b[32m          [ 66%]\u001b[0m\n",
            "test/test_averaging_yet_again.py::test_that_average_of_nothing_still_does_what_is_expected \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Markers\n",
        "In a **large codebase**, I have seen **automated tests take upwards of 30 minutes** to run because there are **so many**, each atomic, each \"arranging\" its setup. It may be beneficial to start using [markers](https://pytest-with-eric.com/pytest-best-practices/pytest-markers/) straight out of the gate so you can run only the tests in the scope you are modifying.\n",
        "\n",
        "This will also come in handy if you are deploying to multiple platforms and your test is only relevant to **specific architectures** or **specific versions** of dependencies. You can then configure CI/CD to run only a subset of tests depending which version you are building.\n",
        "\n",
        "**Markers** allow you to group your unit tests and run them by marker.\n",
        "\n",
        "Lets apply markers to 2 of the 3 tests and see what happens."
      ],
      "metadata": {
        "collapsed": false,
        "id": "1732189433a1cdfb"
      },
      "id": "1732189433a1cdfb"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test/test_averaging_yet_again.py\n",
        "import pytest\n",
        "from gradebook.grade_utils import calculate_average\n",
        "\n",
        "@pytest.mark.average\n",
        "def test_that_average_of_nothing_still_does_what_is_expected():\n",
        "    assert calculate_average(None) is None"
      ],
      "metadata": {
        "id": "GHfkTIR4g6kS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f6b6708-5662-4179-cb34-1de9658613d4"
      },
      "id": "GHfkTIR4g6kS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test/test_averaging_yet_again.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test/test_averaging_some_more.py\n",
        "import pytest\n",
        "from gradebook.grade_utils import calculate_average\n",
        "\n",
        "@pytest.fixture\n",
        "def no_grades():\n",
        "    return []\n",
        "\n",
        "@pytest.mark.average\n",
        "def test_that_average_of_no_grades_still_works(no_grades):\n",
        "    assert calculate_average(no_grades) is None"
      ],
      "metadata": {
        "id": "csHvQhCrhSaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb16bd3-6d94-46fa-b2c8-30d0495a6aae"
      },
      "id": "csHvQhCrhSaY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test/test_averaging_some_more.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All three of our tests will still run with no parameters passed."
      ],
      "metadata": {
        "id": "Ky6rNSIRhpni"
      },
      "id": "Ky6rNSIRhpni"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest"
      ],
      "metadata": {
        "id": "hHLhe9ePhh0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca9e9de-adfd-4985-e92c-6e358030a5f9"
      },
      "id": "hHLhe9ePhh0m",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items                                                                                  \u001b[0m\n",
            "\n",
            "test/test_averaging.py \u001b[32m.\u001b[0m\u001b[33m                                                                     [ 33%]\u001b[0m\n",
            "test/test_averaging_some_more.py \u001b[32m.\u001b[0m\u001b[33m                                                           [ 66%]\u001b[0m\n",
            "test/test_averaging_yet_again.py \u001b[32m.\u001b[0m\u001b[33m                                                           [100%]\u001b[0m\n",
            "\n",
            "\u001b[33m========================================= warnings summary =========================================\u001b[0m\n",
            "test/test_averaging_some_more.py:8\n",
            "  /content/test/test_averaging_some_more.py:8: PytestUnknownMarkWarning: Unknown pytest.mark.average - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
            "    @pytest.mark.average\n",
            "\n",
            "test/test_averaging_yet_again.py:4\n",
            "  /content/test/test_averaging_yet_again.py:4: PytestUnknownMarkWarning: Unknown pytest.mark.average - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
            "    @pytest.mark.average\n",
            "\n",
            "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
            "\u001b[33m================================== \u001b[32m3 passed\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 0.03s\u001b[0m\u001b[33m ===================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But if we pass the marker parameter with the marker name \"average\"..."
      ],
      "metadata": {
        "id": "5ifUfaGChulV"
      },
      "id": "5ifUfaGChulV"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -m average"
      ],
      "metadata": {
        "id": "JuKf8W4Ph8aP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82835825-a3ed-4b11-bcea-e80c4324e6d7"
      },
      "id": "JuKf8W4Ph8aP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items / 1 deselected / 2 selected                                                      \u001b[0m\n",
            "\n",
            "test/test_averaging_some_more.py \u001b[32m.\u001b[0m\u001b[33m                                                           [ 50%]\u001b[0m\n",
            "test/test_averaging_yet_again.py \u001b[32m.\u001b[0m\u001b[33m                                                           [100%]\u001b[0m\n",
            "\n",
            "\u001b[33m========================================= warnings summary =========================================\u001b[0m\n",
            "test/test_averaging_some_more.py:8\n",
            "  /content/test/test_averaging_some_more.py:8: PytestUnknownMarkWarning: Unknown pytest.mark.average - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
            "    @pytest.mark.average\n",
            "\n",
            "test/test_averaging_yet_again.py:4\n",
            "  /content/test/test_averaging_yet_again.py:4: PytestUnknownMarkWarning: Unknown pytest.mark.average - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
            "    @pytest.mark.average\n",
            "\n",
            "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
            "\u001b[33m=========================== \u001b[32m2 passed\u001b[0m, \u001b[33m\u001b[1m1 deselected\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 0.02s\u001b[0m\u001b[33m ============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's just a warning, but we should really register our markers, if only to keep a record of what markers are available when other engineers run our tests and want to find out what markers there are to choose from:"
      ],
      "metadata": {
        "id": "DvfgeRD-imAG"
      },
      "id": "DvfgeRD-imAG"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest --markers"
      ],
      "metadata": {
        "id": "xk3WUgi8iwpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78dea0a-041a-4f89-ebef-d78a5f57832c"
      },
      "id": "xk3WUgi8iwpN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m@pytest.mark.anyio:\u001b[0m mark the (coroutine function) test to be run asynchronously via anyio.\n",
            "\n",
            "\u001b[1m@pytest.mark.filterwarnings(warning):\u001b[0m add a warning filter to the given test. see https://docs.pytest.org/en/stable/how-to/capture-warnings.html#pytest-mark-filterwarnings \n",
            "\n",
            "\u001b[1m@pytest.mark.skip(reason=None):\u001b[0m skip the given test function with an optional reason. Example: skip(reason=\"no way of currently testing this\") skips the test.\n",
            "\n",
            "\u001b[1m@pytest.mark.skipif(condition, ..., *, reason=...):\u001b[0m skip the given test function if any of the conditions evaluate to True. Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. See https://docs.pytest.org/en/stable/reference/reference.html#pytest-mark-skipif\n",
            "\n",
            "\u001b[1m@pytest.mark.xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict):\u001b[0m mark the test function as an expected failure if any of the conditions evaluate to True. Optionally specify a reason for better reporting and run=False if you don't even want to execute the test function. If only specific exception(s) are expected, you can list them in raises, and if the test fails in other ways, it will be reported as a true failure. See https://docs.pytest.org/en/stable/reference/reference.html#pytest-mark-xfail\n",
            "\n",
            "\u001b[1m@pytest.mark.parametrize(argnames, argvalues):\u001b[0m call a test function multiple times passing in different arguments in turn. argvalues generally needs to be a list of values if argnames specifies only one name or a list of tuples of values if argnames specifies multiple names. Example: @parametrize('arg1', [1,2]) would lead to two calls of the decorated test function, one with arg1=1 and another with arg1=2.see https://docs.pytest.org/en/stable/how-to/parametrize.html for more info and examples.\n",
            "\n",
            "\u001b[1m@pytest.mark.usefixtures(fixturename1, fixturename2, ...):\u001b[0m mark tests as needing all of the specified fixtures. see https://docs.pytest.org/en/stable/explanation/fixtures.html#usefixtures \n",
            "\n",
            "\u001b[1m@pytest.mark.tryfirst:\u001b[0m mark a hook implementation function such that the plugin machinery will try to call it first/as early as possible. DEPRECATED, use @pytest.hookimpl(tryfirst=True) instead.\n",
            "\n",
            "\u001b[1m@pytest.mark.trylast:\u001b[0m mark a hook implementation function such that the plugin machinery will try to call it last/as late as possible. DEPRECATED, use @pytest.hookimpl(trylast=True) instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can add our custom marker to a dedicated `pytest.ini` file or a `pyproject.toml` file. I would recommend adding to `pyproject.toml` if there is one. It's always good to go best practice instead of ignoring these warnings so let's configure a `pyproject.toml` so we stop seeing this warning in subsequent runs."
      ],
      "metadata": {
        "id": "9jr2vWc0jN6t"
      },
      "id": "9jr2vWc0jN6t"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pyproject.toml\n",
        "[tool.pytest.ini_options]\n",
        "markers = [\n",
        "    \"average: used to mark all tests associated with averaging grades\"\n",
        "]"
      ],
      "metadata": {
        "id": "nAgIj_BGjqPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b0edfc-739f-405d-db92-bf78eb0e71f2"
      },
      "id": "nAgIj_BGjqPb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pyproject.toml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest --markers"
      ],
      "metadata": {
        "id": "KFmH6mdYkFvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b324e2-c8e8-4b4d-c317-8dfdbf77cc6a"
      },
      "id": "KFmH6mdYkFvW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m@pytest.mark.average:\u001b[0m used to mark all tests associated with averaging grades\n",
            "\n",
            "\u001b[1m@pytest.mark.anyio:\u001b[0m mark the (coroutine function) test to be run asynchronously via anyio.\n",
            "\n",
            "\u001b[1m@pytest.mark.filterwarnings(warning):\u001b[0m add a warning filter to the given test. see https://docs.pytest.org/en/stable/how-to/capture-warnings.html#pytest-mark-filterwarnings \n",
            "\n",
            "\u001b[1m@pytest.mark.skip(reason=None):\u001b[0m skip the given test function with an optional reason. Example: skip(reason=\"no way of currently testing this\") skips the test.\n",
            "\n",
            "\u001b[1m@pytest.mark.skipif(condition, ..., *, reason=...):\u001b[0m skip the given test function if any of the conditions evaluate to True. Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. See https://docs.pytest.org/en/stable/reference/reference.html#pytest-mark-skipif\n",
            "\n",
            "\u001b[1m@pytest.mark.xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict):\u001b[0m mark the test function as an expected failure if any of the conditions evaluate to True. Optionally specify a reason for better reporting and run=False if you don't even want to execute the test function. If only specific exception(s) are expected, you can list them in raises, and if the test fails in other ways, it will be reported as a true failure. See https://docs.pytest.org/en/stable/reference/reference.html#pytest-mark-xfail\n",
            "\n",
            "\u001b[1m@pytest.mark.parametrize(argnames, argvalues):\u001b[0m call a test function multiple times passing in different arguments in turn. argvalues generally needs to be a list of values if argnames specifies only one name or a list of tuples of values if argnames specifies multiple names. Example: @parametrize('arg1', [1,2]) would lead to two calls of the decorated test function, one with arg1=1 and another with arg1=2.see https://docs.pytest.org/en/stable/how-to/parametrize.html for more info and examples.\n",
            "\n",
            "\u001b[1m@pytest.mark.usefixtures(fixturename1, fixturename2, ...):\u001b[0m mark tests as needing all of the specified fixtures. see https://docs.pytest.org/en/stable/explanation/fixtures.html#usefixtures \n",
            "\n",
            "\u001b[1m@pytest.mark.tryfirst:\u001b[0m mark a hook implementation function such that the plugin machinery will try to call it first/as early as possible. DEPRECATED, use @pytest.hookimpl(tryfirst=True) instead.\n",
            "\n",
            "\u001b[1m@pytest.mark.trylast:\u001b[0m mark a hook implementation function such that the plugin machinery will try to call it last/as late as possible. DEPRECATED, use @pytest.hookimpl(trylast=True) instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Neat!\n",
        "\n",
        "There are many other configuration settings for `pytest`. Read more about them in the [official docs](https://docs.pytest.org/en/7.1.x/reference/reference.html#ini-options-ref).\n",
        "\n",
        "Now lets see if the warning went away."
      ],
      "metadata": {
        "id": "yTyJhxmykHUJ"
      },
      "id": "yTyJhxmykHUJ"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -m average"
      ],
      "metadata": {
        "id": "8a6zYyDAkNtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "520f8e96-90ca-4a57-db05-a9aede50753e"
      },
      "id": "8a6zYyDAkNtC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "configfile: pyproject.toml\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items / 1 deselected / 2 selected                                                      \u001b[0m\n",
            "\n",
            "test/test_averaging_some_more.py \u001b[32m.\u001b[0m\u001b[32m                                                           [ 50%]\u001b[0m\n",
            "test/test_averaging_yet_again.py \u001b[32m.\u001b[0m\u001b[32m                                                           [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m================================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1 deselected\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ==================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Love it! You can also **exclude** a marker instead of including it by quoting the marker and preceding with a `not`, e.g.:"
      ],
      "metadata": {
        "id": "1y6AAp6RkTjQ"
      },
      "id": "1y6AAp6RkTjQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -m \"not average\""
      ],
      "metadata": {
        "id": "1thHqA-TkaSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e09be3e-3d06-42a8-e1fa-e114a7ebc941"
      },
      "id": "1thHqA-TkaSv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "configfile: pyproject.toml\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items / 2 deselected / 1 selected                                                      \u001b[0m\n",
            "\n",
            "test/test_averaging.py \u001b[32m.\u001b[0m\u001b[32m                                                                     [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m================================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m2 deselected\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ==================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets go back and add that `average` marker to the 3rd test to be consistent."
      ],
      "metadata": {
        "id": "k-fANXc_ksZw"
      },
      "id": "k-fANXc_ksZw"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test/test_averaging.py\n",
        "import pytest\n",
        "from gradebook.grade_utils import calculate_average\n",
        "\n",
        "@pytest.fixture\n",
        "def some_grades():\n",
        "    return [90, 80, 70]\n",
        "\n",
        "@pytest.mark.average\n",
        "def test_that_average_grade_returns_average_of_grades_provided(some_grades):\n",
        "    assert calculate_average(some_grades) == 80"
      ],
      "metadata": {
        "id": "mbBt6toQhabK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2629d601-bfff-4090-8ec6-ac43b5d44a41"
      },
      "id": "mbBt6toQhabK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test/test_averaging.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test that it worked. We should have no tests that are not marked average."
      ],
      "metadata": {
        "id": "-FNFF7I4gw5y"
      },
      "id": "-FNFF7I4gw5y"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -m \"not average\""
      ],
      "metadata": {
        "id": "rKSxWJD3kzs8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df64544-3e25-41ec-a592-188eb9fe58dc"
      },
      "id": "rKSxWJD3kzs8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "configfile: pyproject.toml\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items / 3 deselected / 0 selected                                                      \u001b[0m\n",
            "\n",
            "\u001b[33m====================================== \u001b[33m\u001b[1m3 deselected\u001b[0m\u001b[33m in 0.02s\u001b[0m\u001b[33m =======================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets add a few more tests so we can see how fixtures can be shared. Here I will borrow the `some_grades` fixture from earlier in our new tests."
      ],
      "metadata": {
        "id": "gKIle7ANhcKS"
      },
      "id": "gKIle7ANhcKS"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test/test_highest_lowest.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test/test_highest_lowest.py\n",
        "import pytest\n",
        "from gradebook.grade_utils import find_highest_grade, find_lowest_grade\n",
        "\n",
        "@pytest.mark.hilo\n",
        "@pytest.mark.highest\n",
        "def test_find_highest_grade(some_grades):\n",
        "    assert find_highest_grade(some_grades) == 90\n",
        "\n",
        "@pytest.mark.hilo\n",
        "@pytest.mark.lowest\n",
        "def test_find_lowest_grade(some_grades):\n",
        "    assert find_lowest_grade(some_grades) == 70"
      ],
      "metadata": {
        "id": "a9e01c2f527e81ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba6858c-1407-4e21-cee4-3e5594860457"
      },
      "id": "a9e01c2f527e81ec",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we are applying **multiple markers** to our new tests.\n",
        "\n",
        "I will go ahead and create the logic as well."
      ],
      "metadata": {
        "id": "puLCTY4Yn-In"
      },
      "id": "puLCTY4Yn-In"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gradebook/grade_utils.py\n",
        "\n",
        "def calculate_average(grades):\n",
        "    \"\"\"Return the average grade\"\"\"\n",
        "    if not grades:\n",
        "      return None\n",
        "    return sum(grades) / len(grades)\n",
        "\n",
        "\n",
        "def find_highest_grade(grades):\n",
        "    \"\"\"Return the highest grade\"\"\"\n",
        "    if not grades:\n",
        "        return None\n",
        "    return max(grades)\n",
        "\n",
        "\n",
        "def find_lowest_grade(grades):\n",
        "    \"\"\"Return the lowest grade\"\"\"\n",
        "    if not grades:\n",
        "        return None\n",
        "    return min(grades)\n"
      ],
      "metadata": {
        "id": "7vb1Nn2An9TS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74082bc0-44bd-4185-a484-ebeac9327615"
      },
      "id": "7vb1Nn2An9TS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting gradebook/grade_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "configfile: pyproject.toml\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 5 items                                                                                  \u001b[0m\n",
            "\n",
            "test/test_averaging.py \u001b[32m.\u001b[0m\u001b[33m                                                                     [ 20%]\u001b[0m\n",
            "test/test_averaging_some_more.py \u001b[32m.\u001b[0m\u001b[33m                                                           [ 40%]\u001b[0m\n",
            "test/test_averaging_yet_again.py \u001b[32m.\u001b[0m\u001b[33m                                                           [ 60%]\u001b[0m\n",
            "test/test_highest_lowest.py \u001b[31mE\u001b[0m\u001b[31mE\u001b[0m\u001b[31m                                                               [100%]\u001b[0m\n",
            "\n",
            "============================================== ERRORS ==============================================\n",
            "\u001b[31m\u001b[1m____________________________ ERROR at setup of test_find_highest_grade _____________________________\u001b[0m\n",
            "file /content/test/test_highest_lowest.py, line 4\n",
            "  @pytest.mark.hilo\n",
            "  @pytest.mark.highest\n",
            "  def test_find_highest_grade(some_grades):\n",
            "\u001b[31mE       fixture 'some_grades' not found\u001b[0m\n",
            "\u001b[31m>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory\u001b[0m\n",
            "\u001b[31m>       use 'pytest --fixtures [testpath]' for help on them.\u001b[0m\n",
            "\n",
            "/content/test/test_highest_lowest.py:4\n",
            "\u001b[31m\u001b[1m_____________________________ ERROR at setup of test_find_lowest_grade _____________________________\u001b[0m\n",
            "file /content/test/test_highest_lowest.py, line 9\n",
            "  @pytest.mark.hilo\n",
            "  @pytest.mark.lowest\n",
            "  def test_find_lowest_grade(some_grades):\n",
            "\u001b[31mE       fixture 'some_grades' not found\u001b[0m\n",
            "\u001b[31m>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory\u001b[0m\n",
            "\u001b[31m>       use 'pytest --fixtures [testpath]' for help on them.\u001b[0m\n",
            "\n",
            "/content/test/test_highest_lowest.py:9\n",
            "\u001b[33m========================================= warnings summary =========================================\u001b[0m\n",
            "test/test_highest_lowest.py:4\n",
            "  /content/test/test_highest_lowest.py:4: PytestUnknownMarkWarning: Unknown pytest.mark.hilo - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
            "    @pytest.mark.hilo\n",
            "\n",
            "test/test_highest_lowest.py:5\n",
            "  /content/test/test_highest_lowest.py:5: PytestUnknownMarkWarning: Unknown pytest.mark.highest - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
            "    @pytest.mark.highest\n",
            "\n",
            "test/test_highest_lowest.py:9\n",
            "  /content/test/test_highest_lowest.py:9: PytestUnknownMarkWarning: Unknown pytest.mark.hilo - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
            "    @pytest.mark.hilo\n",
            "\n",
            "test/test_highest_lowest.py:10\n",
            "  /content/test/test_highest_lowest.py:10: PytestUnknownMarkWarning: Unknown pytest.mark.lowest - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
            "    @pytest.mark.lowest\n",
            "\n",
            "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mERROR\u001b[0m test/test_highest_lowest.py::\u001b[1mtest_find_highest_grade\u001b[0m\n",
            "\u001b[31mERROR\u001b[0m test/test_highest_lowest.py::\u001b[1mtest_find_lowest_grade\u001b[0m\n",
            "\u001b[31m============================= \u001b[32m3 passed\u001b[0m, \u001b[33m4 warnings\u001b[0m, \u001b[31m\u001b[1m2 errors\u001b[0m\u001b[31m in 0.03s\u001b[0m\u001b[31m ==============================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m pytest"
      ],
      "metadata": {
        "id": "197bd456c71d3a29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a976e9d3-7410-42e8-c849-33da31cef352"
      },
      "id": "197bd456c71d3a29",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, so we have a few **new markers** that we will register, but the only **error** is that our fixture is `not found`. Lets address that fixture first. We can share fixtures with all of our tests. Lets move `some_grades` to a `conftest.py` (`pytest` does expect the shared fixtures to live in a file of this specific name) so that we can have a place to add fixtures as our codebase grows."
      ],
      "metadata": {
        "id": "hpgm9F5vopps"
      },
      "id": "hpgm9F5vopps"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test/conftest.py\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture(scope=\"function\")\n",
        "def some_grades():\n",
        "    return [90, 80, 70]"
      ],
      "metadata": {
        "id": "IfAsffoYqJ7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f359040-3dbb-4088-8c6c-42e0c92396ce"
      },
      "id": "IfAsffoYqJ7M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test/conftest.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When fixtures are defined in `conftest.py`, they can have a **scope** applied. The scope determines when the fixture will be invoked. In this case, this fixture will be invoked for each function in which it is used. This is actually the **default**.\n",
        "\n",
        "There are several options because some fixtures are complex. You can instantiate a class or a service as a fixture, and you will not want **expensive operations** to be performed more often than necessary.\n",
        "\n",
        "The other options are:\n",
        "\n",
        "- class\n",
        "- module\n",
        "- session\n",
        "\n",
        "[Real world example](https://github.com/getsentry/sentry/blob/a471c8ad53dcd956f6b886c06ee1555697966002/src/sentry/testutils/pytest/kafka.py#L14.) of why you might want to scope fixtures."
      ],
      "metadata": {
        "id": "011cH189jcC6"
      },
      "id": "011cH189jcC6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets **remove** the fixture from where it had originally been declared."
      ],
      "metadata": {
        "id": "WdNAE3bCRbFb"
      },
      "id": "WdNAE3bCRbFb"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test/test_averaging.py\n",
        "import pytest\n",
        "from gradebook.grade_utils import calculate_average\n",
        "\n",
        "@pytest.mark.average\n",
        "def test_that_average_grade_returns_average_of_grades_provided(some_grades):\n",
        "    assert calculate_average(some_grades) == 80"
      ],
      "metadata": {
        "id": "OaCPrO9YrbDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922882f8-5327-4c9c-c0cd-3d03e1897495"
      },
      "id": "OaCPrO9YrbDA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test/test_averaging.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets also **register our new markers**."
      ],
      "metadata": {
        "id": "QZeUuidvqioE"
      },
      "id": "QZeUuidvqioE"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pyproject.toml\n",
        "[tool.pytest.ini_options]\n",
        "markers = [\n",
        "    \"average: used to mark all tests associated with averaging grades\",\n",
        "    \"hilo: used to mark tests associated with identifying the extreme grades\",\n",
        "    \"highest: used to mark tests associated with identfying the highest grade\",\n",
        "    \"lowest: used to mark tests associated with identifying the lowest grade\"\n",
        "]"
      ],
      "metadata": {
        "id": "BfbWwXTHojZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8658ed1-ab28-4a98-aca9-d4f7c9a2f880"
      },
      "id": "BfbWwXTHojZ7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pyproject.toml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying **multiple markers** to the same tests, we can now refer to any of the markers associated with a test to run that test."
      ],
      "metadata": {
        "id": "5lIlE3M6pViO"
      },
      "id": "5lIlE3M6pViO"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -m hilo -v"
      ],
      "metadata": {
        "id": "A8XH-Jxophgo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14add2d6-b57e-4269-fa30-0524d0b212ae"
      },
      "id": "A8XH-Jxophgo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "configfile: pyproject.toml\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 5 items / 3 deselected / 2 selected                                                      \u001b[0m\n",
            "\n",
            "test/test_highest_lowest.py::test_find_highest_grade \u001b[32mPASSED\u001b[0m\u001b[32m                                  [ 50%]\u001b[0m\n",
            "test/test_highest_lowest.py::test_find_lowest_grade \u001b[32mPASSED\u001b[0m\u001b[32m                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m================================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m3 deselected\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ==================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both of the tests in the hilo file ran!\n",
        "\n",
        "And to demonstrate the point that we can use **any of the markers** applied to a test to run that test..."
      ],
      "metadata": {
        "id": "BhP2gnlbujyw"
      },
      "id": "BhP2gnlbujyw"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -m highest -v"
      ],
      "metadata": {
        "id": "ALmf-GOXuqjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c54584-7dee-468b-c603-606d9f2ffe63"
      },
      "id": "ALmf-GOXuqjz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "configfile: pyproject.toml\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 5 items / 4 deselected / 1 selected                                                      \u001b[0m\n",
            "\n",
            "test/test_highest_lowest.py::test_find_highest_grade \u001b[32mPASSED\u001b[0m\u001b[32m                                  [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m================================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m4 deselected\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ==================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lots of marker settings available, including **conditional skipping** and **intentional failure**, which can be used as a placeholder to address an issue, and **timeout** markers that will cause the test to error if it exceeds the set time.\n",
        "\n",
        "More examples with explanations can be found on this handy blog: https://pytest-with-eric.com/pytest-best-practices/pytest-markers/\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "f561b41c6079f2c2"
      },
      "id": "f561b41c6079f2c2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Markers can also be used to [skip tests based on conditions](https://github.com/urllib3/urllib3/blob/da410581b6b3df73da976b5ce5eb20a4bd030437/dummyserver/testcase.py#L314)."
      ],
      "metadata": {
        "id": "D-inP5afas8E"
      },
      "id": "D-inP5afas8E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we add more tests, some additional considerations...\n",
        "\n",
        "* Tests should not be **interdependent**.\n",
        "* Tests need to be able to run **in any order**.\n",
        "* Unit tests should have a low **execution time** because there will be many of them.\n",
        "* Tests need to be run **frequently**, preferably **automatically** as a pre-commit hook or, much better, as part of a CI workflow."
      ],
      "metadata": {
        "collapsed": false,
        "id": "2cf6a81fd549ce57"
      },
      "id": "2cf6a81fd549ce57"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest --durations=0 -vv"
      ],
      "metadata": {
        "id": "3-qlT1OtgQHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee09c8ed-0256-46c2-b2ea-821924ad2cd4"
      },
      "id": "3-qlT1OtgQHp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "configfile: pyproject.toml\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 5 items                                                                                  \u001b[0m\n",
            "\n",
            "test/test_averaging.py::test_that_average_grade_returns_average_of_grades_provided \u001b[32mPASSED\u001b[0m\u001b[32m    [ 20%]\u001b[0m\n",
            "test/test_averaging_some_more.py::test_that_average_of_no_grades_still_works \u001b[32mPASSED\u001b[0m\u001b[32m          [ 40%]\u001b[0m\n",
            "test/test_averaging_yet_again.py::test_that_average_of_nothing_still_does_what_is_expected \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
            "test/test_highest_lowest.py::test_find_highest_grade \u001b[32mPASSED\u001b[0m\u001b[32m                                  [ 80%]\u001b[0m\n",
            "test/test_highest_lowest.py::test_find_lowest_grade \u001b[32mPASSED\u001b[0m\u001b[32m                                   [100%]\u001b[0m\n",
            "\n",
            "======================================== slowest durations =========================================\n",
            "0.00s setup    test/test_averaging.py::test_that_average_grade_returns_average_of_grades_provided\n",
            "0.00s setup    test/test_highest_lowest.py::test_find_highest_grade\n",
            "0.00s teardown test/test_averaging_some_more.py::test_that_average_of_no_grades_still_works\n",
            "0.00s setup    test/test_averaging_some_more.py::test_that_average_of_no_grades_still_works\n",
            "0.00s setup    test/test_highest_lowest.py::test_find_lowest_grade\n",
            "0.00s call     test/test_averaging_some_more.py::test_that_average_of_no_grades_still_works\n",
            "0.00s teardown test/test_averaging.py::test_that_average_grade_returns_average_of_grades_provided\n",
            "0.00s call     test/test_averaging_yet_again.py::test_that_average_of_nothing_still_does_what_is_expected\n",
            "0.00s call     test/test_averaging.py::test_that_average_grade_returns_average_of_grades_provided\n",
            "0.00s teardown test/test_highest_lowest.py::test_find_highest_grade\n",
            "0.00s setup    test/test_averaging_yet_again.py::test_that_average_of_nothing_still_does_what_is_expected\n",
            "0.00s teardown test/test_highest_lowest.py::test_find_lowest_grade\n",
            "0.00s teardown test/test_averaging_yet_again.py::test_that_average_of_nothing_still_does_what_is_expected\n",
            "0.00s call     test/test_highest_lowest.py::test_find_highest_grade\n",
            "0.00s call     test/test_highest_lowest.py::test_find_lowest_grade\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets add some more tests and some more functions and then rerun our new test suite."
      ],
      "metadata": {
        "collapsed": false,
        "id": "cd744bc7ee9f0c82"
      },
      "id": "cd744bc7ee9f0c82"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test/test_integrated_functionality.py\n",
        "import pytest\n",
        "from gradebook.grade_utils import calculate_average, determine_letter_grade\n",
        "\n",
        "def test_letter_grade_average(some_grades):\n",
        "    # calculate_average\n",
        "    average = calculate_average(some_grades)\n",
        "\n",
        "    # determine_letter_grade for the average\n",
        "    average_letter_grade = determine_letter_grade(average)\n",
        "    assert average_letter_grade == \"B\""
      ],
      "metadata": {
        "id": "FXOTWFftf0dD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbbf87b0-6e01-44de-e0c3-8e93be8a5cfb"
      },
      "id": "FXOTWFftf0dD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test/test_integrated_functionality.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### That's technically a super lightweight **integration** test because it checks that two different parts of our codebase work together."
      ],
      "metadata": {
        "collapsed": false,
        "id": "166debb8921957d4"
      },
      "id": "166debb8921957d4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add the code to return a letter grade for a numeric grade."
      ],
      "metadata": {
        "id": "EOwQ4Q_xmhph"
      },
      "id": "EOwQ4Q_xmhph"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat << _EOF >> gradebook/grade_utils.py\n",
        "\n",
        "\n",
        "def determine_letter_grade(grade):\n",
        "    \"\"\"Return the letter grade for a numeric grade.\"\"\"\n",
        "    if grade >= 90:\n",
        "        return \"A\"\n",
        "    elif grade >= 80:\n",
        "        return \"B\"\n",
        "    elif grade >= 70:\n",
        "        return \"C\"\n",
        "    elif grade >= 60:\n",
        "        return \"D\"\n",
        "    else:\n",
        "        return \"F\"\n",
        "_EOF"
      ],
      "metadata": {
        "id": "N4BYm2tGlfai"
      },
      "id": "N4BYm2tGlfai",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "configfile: pyproject.toml\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 6 items                                                                                  \u001b[0m\n",
            "\n",
            "test/test_averaging.py \u001b[32m.\u001b[0m\u001b[32m                                                                     [ 16%]\u001b[0m\n",
            "test/test_averaging_some_more.py \u001b[32m.\u001b[0m\u001b[32m                                                           [ 33%]\u001b[0m\n",
            "test/test_averaging_yet_again.py \u001b[32m.\u001b[0m\u001b[32m                                                           [ 50%]\u001b[0m\n",
            "test/test_highest_lowest.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                               [ 83%]\u001b[0m\n",
            "test/test_integrated_functionality.py \u001b[32m.\u001b[0m\u001b[32m                                                      [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m6 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m pytest"
      ],
      "metadata": {
        "id": "6ab32cfdedf2b9e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0b74632-598e-40cd-e68a-d6e9e92ee948"
      },
      "id": "6ab32cfdedf2b9e3",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Parametrization\n",
        "Now that we've added the letter grade function, it would be a great time to bring up **parametrization**. Parametrization comes in handy if you find yourself writing a lot of tests that look very similar, or if you have to test for a long list of scenarios that could be easily expressed as parameters."
      ],
      "metadata": {
        "id": "3uWXN2RHvVcH"
      },
      "id": "3uWXN2RHvVcH"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test/test_parametrized_letter_grades.py\n",
        "import pytest\n",
        "\n",
        "from gradebook.grade_utils import determine_letter_grade\n",
        "\n",
        "\n",
        "grade_ranges = {\n",
        "    \"A\": range(90, 101),\n",
        "    \"B\": range(80, 90),\n",
        "    \"C\": range(70, 80),\n",
        "    \"D\": range(60, 70),\n",
        "    \"F\": range(0, 60),\n",
        "}\n",
        "\n",
        "@pytest.mark.parametrize(\"letter,number\",\n",
        "                         [(letter, number) for letter, numbers in grade_ranges.items() for number in numbers])\n",
        "\n",
        "def test_is_letter_grade(letter, number):\n",
        "    assert determine_letter_grade(number) == letter"
      ],
      "metadata": {
        "id": "MmqFAAgPv2WU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4ba8541-1db7-4004-a393-6e8ca11645ce"
      },
      "id": "MmqFAAgPv2WU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test/test_parametrized_letter_grades.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Brace yourself..."
      ],
      "metadata": {
        "id": "nDuMQSoHv_WZ"
      },
      "id": "nDuMQSoHv_WZ"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest"
      ],
      "metadata": {
        "id": "8ALitQF1wDdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180aeab8-9610-4157-b117-f158896d43e1"
      },
      "id": "8ALitQF1wDdn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "configfile: pyproject.toml\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 107 items                                                                                \u001b[0m\n",
            "\n",
            "test/test_averaging.py \u001b[32m.\u001b[0m\u001b[32m                                                                     [  0%]\u001b[0m\n",
            "test/test_averaging_some_more.py \u001b[32m.\u001b[0m\u001b[32m                                                           [  1%]\u001b[0m\n",
            "test/test_averaging_yet_again.py \u001b[32m.\u001b[0m\u001b[32m                                                           [  2%]\u001b[0m\n",
            "test/test_highest_lowest.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                               [  4%]\u001b[0m\n",
            "test/test_integrated_functionality.py \u001b[32m.\u001b[0m\u001b[32m                                                      [  5%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 54%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                            [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================= \u001b[32m\u001b[1m107 passed\u001b[0m\u001b[32m in 0.23s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -v"
      ],
      "metadata": {
        "id": "5Kqtt5W2kHlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18320e60-0430-4dae-e0a7-c30f5a510887"
      },
      "id": "5Kqtt5W2kHlG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "configfile: pyproject.toml\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 107 items                                                                                \u001b[0m\n",
            "\n",
            "test/test_averaging.py::test_that_average_grade_returns_average_of_grades_provided \u001b[32mPASSED\u001b[0m\u001b[32m    [  0%]\u001b[0m\n",
            "test/test_averaging_some_more.py::test_that_average_of_no_grades_still_works \u001b[32mPASSED\u001b[0m\u001b[32m          [  1%]\u001b[0m\n",
            "test/test_averaging_yet_again.py::test_that_average_of_nothing_still_does_what_is_expected \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
            "test/test_highest_lowest.py::test_find_highest_grade \u001b[32mPASSED\u001b[0m\u001b[32m                                  [  3%]\u001b[0m\n",
            "test/test_highest_lowest.py::test_find_lowest_grade \u001b[32mPASSED\u001b[0m\u001b[32m                                   [  4%]\u001b[0m\n",
            "test/test_integrated_functionality.py::test_letter_grade_average \u001b[32mPASSED\u001b[0m\u001b[32m                      [  5%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[A-90] \u001b[32mPASSED\u001b[0m\u001b[32m                   [  6%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[A-91] \u001b[32mPASSED\u001b[0m\u001b[32m                   [  7%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[A-92] \u001b[32mPASSED\u001b[0m\u001b[32m                   [  8%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[A-93] \u001b[32mPASSED\u001b[0m\u001b[32m                   [  9%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[A-94] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 10%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[A-95] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 11%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[A-96] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 12%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[A-97] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 13%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[A-98] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 14%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[A-99] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 14%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[A-100] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 15%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[B-80] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 16%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[B-81] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 17%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[B-82] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 18%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[B-83] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 19%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[B-84] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 20%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[B-85] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 21%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[B-86] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 22%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[B-87] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 23%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[B-88] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 24%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[B-89] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 25%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[C-70] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 26%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[C-71] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 27%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[C-72] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 28%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[C-73] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 28%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[C-74] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 29%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[C-75] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 30%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[C-76] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 31%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[C-77] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 32%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[C-78] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 33%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[C-79] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 34%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[D-60] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 35%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[D-61] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 36%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[D-62] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 37%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[D-63] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 38%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[D-64] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 39%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[D-65] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 40%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[D-66] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 41%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[D-67] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 42%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[D-68] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 42%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[D-69] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 43%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-0] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 44%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-1] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 45%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-2] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 46%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-3] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 47%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-4] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 48%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-5] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 49%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-6] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 50%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-7] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 51%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-8] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 52%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-9] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 53%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-10] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 54%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-11] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 55%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-12] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 56%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-13] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 57%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-14] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 57%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-15] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 58%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-16] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 59%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-17] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 60%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-18] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 61%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-19] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 62%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-20] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 63%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-21] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 64%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-22] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 65%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-23] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 66%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-24] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 67%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-25] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 68%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-26] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 69%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-27] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 70%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-28] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 71%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-29] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 71%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-30] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 72%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-31] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 73%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-32] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 74%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-33] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 75%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-34] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 76%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-35] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 77%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-36] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 78%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-37] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 79%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-38] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 80%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-39] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 81%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-40] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 82%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-41] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 83%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-42] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 84%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-43] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 85%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-44] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 85%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-45] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 86%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-46] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 87%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-47] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 88%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-48] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 89%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-49] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 90%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-50] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 91%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-51] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 92%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-52] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 93%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-53] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 94%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-54] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 95%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-55] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 96%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-56] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 97%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-57] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 98%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-58] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 99%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py::test_is_letter_grade[F-59] \u001b[32mPASSED\u001b[0m\u001b[32m                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================= \u001b[32m\u001b[1m107 passed\u001b[0m\u001b[32m in 0.18s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've looked at a silly example, a [real example of parametrization](https://github.com/tiangolo/fastapi/blob/a9819dfd8da39a754837cc134df4aca6c0a9a3f6/tests/test_param_include_in_schema.py#L168)."
      ],
      "metadata": {
        "id": "RNqdmG9fSm8Z"
      },
      "id": "RNqdmG9fSm8Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mocking Complex Dependencies\n",
        "\n",
        "The last feature that will be essential as you begin to work with established codebases is **mocking**. [`pytest-mock`](https://pytest-mock.readthedocs.io/en/latest/usage.html) is a `pytest` plugin that provides mocking functionality. You should not connect to an actual database or your 3rd party API during unit testing. You should \"mock\" these dependencies instead.\n",
        "\n",
        "### Why mock?\n",
        "Because unit tests should be isolated, have **no dependencies**, and be **fast**, avoiding latency like that found in networking and database or disk operations\n",
        "\n",
        "You can mock pretty much anything:\n",
        "- HTTP requests\n",
        "- Database query results\n",
        "- File system manipulation\n",
        "- Built-in functions and constants\n",
        "- Complex classes we do not want to have to initialize\n",
        "- 3rd party libraries"
      ],
      "metadata": {
        "collapsed": false,
        "id": "aaee3243dd5bd362"
      },
      "id": "aaee3243dd5bd362"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytest-mock\n",
            "  Downloading pytest_mock-3.14.0-py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.10/dist-packages (from pytest-mock) (7.4.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->pytest-mock) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->pytest-mock) (24.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->pytest-mock) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->pytest-mock) (1.2.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->pytest-mock) (2.0.1)\n",
            "Installing collected packages: pytest-mock\n",
            "Successfully installed pytest-mock-3.14.0\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install pytest-mock"
      ],
      "metadata": {
        "id": "8e54134c294d53e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdebd5dd-cf42-4e78-ca91-95f689fefe86"
      },
      "id": "8e54134c294d53e1",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create a simple function to write the grades list to a file."
      ],
      "metadata": {
        "id": "HGxmgW7-rtyg"
      },
      "id": "HGxmgW7-rtyg"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gradebook/save_grades.py\n",
        "def write_to_file(grades) -> None:\n",
        "    \"\"\"\n",
        "    Function to write our grades to a file\n",
        "    :param grades: grades list\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    with open(f\"grades.txt\", \"w\") as f:\n",
        "        f.write(str(grades))\n"
      ],
      "metadata": {
        "id": "U-TQSe_pragx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c24a7a-a7ea-4042-bcf4-2fbe63af6bcf"
      },
      "id": "U-TQSe_pragx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gradebook/save_grades.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function opens a file and writes the grades. Super simple. We don't want to actually open a file or actually write to a file, so we will mock the opening and writing. We will intercept these calls."
      ],
      "metadata": {
        "id": "eiGlZi1gr8uh"
      },
      "id": "eiGlZi1gr8uh"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test/test_write_to_file.py\n",
        "import pytest\n",
        "\n",
        "from gradebook.save_grades import write_to_file\n",
        "\n",
        "# first we pass the mocker in\n",
        "def test_write_grades_to_file(mocker):\n",
        "    \"\"\"\n",
        "    Function to test writing grades to a file\n",
        "    \"\"\"\n",
        "\n",
        "    # mock the 'open' function call to return a file object\n",
        "    # using a builtin from unittest\n",
        "    mock_file = mocker.mock_open()\n",
        "    mocker.patch(\"builtins.open\", mock_file)\n",
        "\n",
        "    # now we can call our function that writes to a file\n",
        "    write_to_file([50,75,100])\n",
        "\n",
        "    # assert that the 'open' function was called with the expected arguments\n",
        "    mock_file.assert_called_once_with(\"grades.txt\", \"w\")\n",
        "\n",
        "    # assert that the file was written to with the expected text\n",
        "    mock_file().write.assert_called_once_with(str([50,75,100]))"
      ],
      "metadata": {
        "id": "kcn1Mu5As4Vj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bcce13-6109-4c61-e6be-026f5fb5da6e"
      },
      "id": "kcn1Mu5As4Vj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test/test_write_to_file.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest"
      ],
      "metadata": {
        "id": "q9aRd_C5uVD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ac81a5-9096-4763-9d89-ac974e17ae49"
      },
      "id": "q9aRd_C5uVD-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "configfile: pyproject.toml\n",
            "plugins: mock-3.14.0, anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 108 items                                                                                \u001b[0m\n",
            "\n",
            "test/test_averaging.py \u001b[32m.\u001b[0m\u001b[32m                                                                     [  0%]\u001b[0m\n",
            "test/test_averaging_some_more.py \u001b[32m.\u001b[0m\u001b[32m                                                           [  1%]\u001b[0m\n",
            "test/test_averaging_yet_again.py \u001b[32m.\u001b[0m\u001b[32m                                                           [  2%]\u001b[0m\n",
            "test/test_highest_lowest.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                               [  4%]\u001b[0m\n",
            "test/test_integrated_functionality.py \u001b[32m.\u001b[0m\u001b[32m                                                      [  5%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 53%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                            [ 99%]\u001b[0m\n",
            "test/test_write_to_file.py \u001b[32m.\u001b[0m\u001b[32m                                                                 [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================= \u001b[32m\u001b[1m108 passed\u001b[0m\u001b[32m in 0.18s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beyond mocks there are also **spies**, but mocks are plenty on their own. Read all about mocking features in the [docs](https://pytest-mock.readthedocs.io/en/latest/usage.html). Mocks replace functionality with hardcoded values and spies replace only portions of real classes/modules which can be useful when you want to make sure that a deeper method in a 3rd party library or legacy codebase was invoked."
      ],
      "metadata": {
        "id": "OHKbt4oP0E3_"
      },
      "id": "OHKbt4oP0E3_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Here's [a real mock example](https://github.com/microsoft/timewarp/blob/44dca8474cb6182458830677763261cffccfaac4/utilities/fixtures.py#L80) from Microsoft.\n",
        "\n",
        "#### And a [real spy example](https://github.com/slackapi/python-slack-events-api/blob/2884d7d21fea634d1e5e7926409ed87f6fcc14cf/tests/test_server.py#L179) from Slack."
      ],
      "metadata": {
        "id": "UFMGfqgXaLY7"
      },
      "id": "UFMGfqgXaLY7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Coverage\n",
        "\n",
        "**Aim** for **high** code coverage, but don't obsess.\n",
        "\n",
        "The coverage **Statement** coverage measures how many statements in the code were executed\n",
        "\n",
        "Lets see what our coverage looks like!"
      ],
      "metadata": {
        "collapsed": false,
        "id": "519a2d58faccc1"
      },
      "id": "519a2d58faccc1"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting coverage\n",
            "  Downloading coverage-7.5.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m231.6/231.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: coverage\n",
            "Successfully installed coverage-7.5.3\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install coverage"
      ],
      "metadata": {
        "id": "a8a00635d9e81f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef5e1813-46f3-469c-87ce-95c2a82e7e9f"
      },
      "id": "a8a00635d9e81f0",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!coverage run -m pytest"
      ],
      "metadata": {
        "id": "2bXtp5LV4ChW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeda402a-5a7e-4833-d782-e9474ca2bcf7"
      },
      "id": "2bXtp5LV4ChW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "configfile: pyproject.toml\n",
            "plugins: mock-3.14.0, anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 108 items                                                                                \u001b[0m\n",
            "\n",
            "test/test_averaging.py \u001b[32m.\u001b[0m\u001b[32m                                                                     [  0%]\u001b[0m\n",
            "test/test_averaging_some_more.py \u001b[32m.\u001b[0m\u001b[32m                                                           [  1%]\u001b[0m\n",
            "test/test_averaging_yet_again.py \u001b[32m.\u001b[0m\u001b[32m                                                           [  2%]\u001b[0m\n",
            "test/test_highest_lowest.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                               [  4%]\u001b[0m\n",
            "test/test_integrated_functionality.py \u001b[32m.\u001b[0m\u001b[32m                                                      [  5%]\u001b[0m\n",
            "test/test_parametrized_letter_grades.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 53%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                            [ 99%]\u001b[0m\n",
            "test/test_write_to_file.py \u001b[32m.\u001b[0m\u001b[32m                                                                 [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================= \u001b[32m\u001b[1m108 passed\u001b[0m\u001b[32m in 0.52s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well that looks exactly like our usual `pytest` output..."
      ],
      "metadata": {
        "id": "n1pXg6sM4SLj"
      },
      "id": "n1pXg6sM4SLj"
    },
    {
      "cell_type": "code",
      "source": [
        "!coverage report -m"
      ],
      "metadata": {
        "id": "LH1y3Edv4Vqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba28db7b-6752-4c4a-82f6-0ae49bdebb10"
      },
      "id": "LH1y3Edv4Vqy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name                                      Stmts   Miss  Cover   Missing\n",
            "-----------------------------------------------------------------------\n",
            "gradebook/__init__.py                         0      0   100%\n",
            "gradebook/grade_utils.py                     22      2    91%   12, 19\n",
            "gradebook/save_grades.py                      3      0   100%\n",
            "test/conftest.py                              4      0   100%\n",
            "test/test_averaging.py                        5      0   100%\n",
            "test/test_averaging_some_more.py              8      0   100%\n",
            "test/test_averaging_yet_again.py              5      0   100%\n",
            "test/test_highest_lowest.py                  10      0   100%\n",
            "test/test_integrated_functionality.py         6      0   100%\n",
            "test/test_parametrized_letter_grades.py       6      0   100%\n",
            "test/test_write_to_file.py                    8      0   100%\n",
            "-----------------------------------------------------------------------\n",
            "TOTAL                                        77      2    97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### That's pretty good!\n",
        "\n",
        "In case you're wondering what those headings mean:  \n",
        "\n",
        "**Stmts** The total number of statements in the package.  \n",
        "**Miss** The number of statements that were not executed during testing.  \n",
        "**Cover** The percentage of statements that were executed during testing."
      ],
      "metadata": {
        "collapsed": false,
        "id": "b4c4df4ab7bcdbc9"
      },
      "id": "b4c4df4ab7bcdbc9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "But say we don't want to have to go poking around in the files trying to figure out where we missed some opportunities to write tests. Say we rather point and click."
      ],
      "metadata": {
        "id": "uSic6lDU4wA1"
      },
      "id": "uSic6lDU4wA1"
    },
    {
      "cell_type": "code",
      "source": [
        "!coverage html"
      ],
      "metadata": {
        "id": "EnMzPVmk5Efq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312f8673-a29e-46d5-fe23-cd687ccc3286"
      },
      "id": "EnMzPVmk5Efq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote HTML report to \u001b]8;;file:///content/htmlcov/index.html\u0007htmlcov/index.html\u001b]8;;\u0007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "file_path = 'htmlcov/index.html'\n",
        "with open(file_path, 'r') as file:\n",
        "    html_content = file.read()\n",
        "display(HTML(html_content))"
      ],
      "metadata": {
        "id": "6-eqFjwL6pk8",
        "colab": {
          "resources": {
            "http://localhost:8080/keybd_closed_cb_ce680311.png": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/coverage_html_cb_6fb7b396.js": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/style_cb_8e611ae1.css": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "outputId": "0132e78a-a0c3-46a3-b1bf-1e268de3c328"
      },
      "id": "6-eqFjwL6pk8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "<head>\n",
              "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
              "    <title>Coverage report</title>\n",
              "    <link rel=\"icon\" sizes=\"32x32\" href=\"favicon_32_cb_58284776.png\">\n",
              "    <link rel=\"stylesheet\" href=\"style_cb_8e611ae1.css\" type=\"text/css\">\n",
              "    <script src=\"coverage_html_cb_6fb7b396.js\" defer></script>\n",
              "</head>\n",
              "<body class=\"indexfile\">\n",
              "<header>\n",
              "    <div class=\"content\">\n",
              "        <h1>Coverage report:\n",
              "            <span class=\"pc_cov\">97%</span>\n",
              "        </h1>\n",
              "        <aside id=\"help_panel_wrapper\">\n",
              "            <input id=\"help_panel_state\" type=\"checkbox\">\n",
              "            <label for=\"help_panel_state\">\n",
              "                <img id=\"keyboard_icon\" src=\"keybd_closed_cb_ce680311.png\" alt=\"Show/hide keyboard shortcuts\">\n",
              "            </label>\n",
              "            <div id=\"help_panel\">\n",
              "                <p class=\"legend\">Shortcuts on this page</p>\n",
              "                <div class=\"keyhelp\">\n",
              "                    <p>\n",
              "                        <kbd>f</kbd>\n",
              "                        <kbd>s</kbd>\n",
              "                        <kbd>m</kbd>\n",
              "                        <kbd>x</kbd>\n",
              "                        <kbd>c</kbd>\n",
              "                        &nbsp; change column sorting\n",
              "                    </p>\n",
              "                    <p>\n",
              "                        <kbd>[</kbd>\n",
              "                        <kbd>]</kbd>\n",
              "                        &nbsp; prev/next file\n",
              "                    </p>\n",
              "                    <p>\n",
              "                        <kbd>?</kbd> &nbsp; show/hide this help\n",
              "                    </p>\n",
              "                </div>\n",
              "            </div>\n",
              "        </aside>\n",
              "        <form id=\"filter_container\">\n",
              "            <input id=\"filter\" type=\"text\" value=\"\" placeholder=\"filter...\">\n",
              "            <div>\n",
              "                <input id=\"hide100\" type=\"checkbox\" >\n",
              "                <label for=\"hide100\">hide covered</label>\n",
              "            </div>\n",
              "        </form>\n",
              "        <h2>\n",
              "                <a class=\"button current\">Files</a>\n",
              "                <a class=\"button\" href=\"function_index.html\">Functions</a>\n",
              "                <a class=\"button\" href=\"class_index.html\">Classes</a>\n",
              "        </h2>\n",
              "        <p class=\"text\">\n",
              "            <a class=\"nav\" href=\"https://coverage.readthedocs.io/en/7.5.3\">coverage.py v7.5.3</a>,\n",
              "            created at 2024-06-06 15:50 +0000\n",
              "        </p>\n",
              "    </div>\n",
              "</header>\n",
              "<main id=\"index\">\n",
              "    <table class=\"index\" data-sortable>\n",
              "        <thead>\n",
              "            <tr class=\"tablehead\" title=\"Click to sort\">\n",
              "                <th id=\"file\" class=\"name left\" aria-sort=\"none\" data-shortcut=\"f\">File<span class=\"arrows\"></span></th>\n",
              "                <th id=\"statements\" aria-sort=\"none\" data-default-sort-order=\"descending\" data-shortcut=\"s\">statements<span class=\"arrows\"></span></th>\n",
              "                <th id=\"missing\" aria-sort=\"none\" data-default-sort-order=\"descending\" data-shortcut=\"m\">missing<span class=\"arrows\"></span></th>\n",
              "                <th id=\"excluded\" aria-sort=\"none\" data-default-sort-order=\"descending\" data-shortcut=\"x\">excluded<span class=\"arrows\"></span></th>\n",
              "                <th id=\"coverage\" class=\"right\" aria-sort=\"none\" data-shortcut=\"c\">coverage<span class=\"arrows\"></span></th>\n",
              "            </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "            <tr class=\"region\">\n",
              "                <td class=\"name left\"><a href=\"z_d0c694b4a484f635___init___py.html\">gradebook/__init__.py</a></td>\n",
              "                <td>0</td>\n",
              "                <td>0</td>\n",
              "                <td>0</td>\n",
              "                <td class=\"right\" data-ratio=\"0 0\">100%</td>\n",
              "            </tr>\n",
              "            <tr class=\"region\">\n",
              "                <td class=\"name left\"><a href=\"z_d0c694b4a484f635_grade_utils_py.html\">gradebook/grade_utils.py</a></td>\n",
              "                <td>22</td>\n",
              "                <td>2</td>\n",
              "                <td>0</td>\n",
              "                <td class=\"right\" data-ratio=\"20 22\">91%</td>\n",
              "            </tr>\n",
              "            <tr class=\"region\">\n",
              "                <td class=\"name left\"><a href=\"z_d0c694b4a484f635_save_grades_py.html\">gradebook/save_grades.py</a></td>\n",
              "                <td>3</td>\n",
              "                <td>0</td>\n",
              "                <td>0</td>\n",
              "                <td class=\"right\" data-ratio=\"3 3\">100%</td>\n",
              "            </tr>\n",
              "            <tr class=\"region\">\n",
              "                <td class=\"name left\"><a href=\"z_36f028580bb02cc8_conftest_py.html\">test/conftest.py</a></td>\n",
              "                <td>4</td>\n",
              "                <td>0</td>\n",
              "                <td>0</td>\n",
              "                <td class=\"right\" data-ratio=\"4 4\">100%</td>\n",
              "            </tr>\n",
              "            <tr class=\"region\">\n",
              "                <td class=\"name left\"><a href=\"z_36f028580bb02cc8_test_averaging_py.html\">test/test_averaging.py</a></td>\n",
              "                <td>5</td>\n",
              "                <td>0</td>\n",
              "                <td>0</td>\n",
              "                <td class=\"right\" data-ratio=\"5 5\">100%</td>\n",
              "            </tr>\n",
              "            <tr class=\"region\">\n",
              "                <td class=\"name left\"><a href=\"z_36f028580bb02cc8_test_averaging_some_more_py.html\">test/test_averaging_some_more.py</a></td>\n",
              "                <td>8</td>\n",
              "                <td>0</td>\n",
              "                <td>0</td>\n",
              "                <td class=\"right\" data-ratio=\"8 8\">100%</td>\n",
              "            </tr>\n",
              "            <tr class=\"region\">\n",
              "                <td class=\"name left\"><a href=\"z_36f028580bb02cc8_test_averaging_yet_again_py.html\">test/test_averaging_yet_again.py</a></td>\n",
              "                <td>5</td>\n",
              "                <td>0</td>\n",
              "                <td>0</td>\n",
              "                <td class=\"right\" data-ratio=\"5 5\">100%</td>\n",
              "            </tr>\n",
              "            <tr class=\"region\">\n",
              "                <td class=\"name left\"><a href=\"z_36f028580bb02cc8_test_highest_lowest_py.html\">test/test_highest_lowest.py</a></td>\n",
              "                <td>10</td>\n",
              "                <td>0</td>\n",
              "                <td>0</td>\n",
              "                <td class=\"right\" data-ratio=\"10 10\">100%</td>\n",
              "            </tr>\n",
              "            <tr class=\"region\">\n",
              "                <td class=\"name left\"><a href=\"z_36f028580bb02cc8_test_integrated_functionality_py.html\">test/test_integrated_functionality.py</a></td>\n",
              "                <td>6</td>\n",
              "                <td>0</td>\n",
              "                <td>0</td>\n",
              "                <td class=\"right\" data-ratio=\"6 6\">100%</td>\n",
              "            </tr>\n",
              "            <tr class=\"region\">\n",
              "                <td class=\"name left\"><a href=\"z_36f028580bb02cc8_test_parametrized_letter_grades_py.html\">test/test_parametrized_letter_grades.py</a></td>\n",
              "                <td>6</td>\n",
              "                <td>0</td>\n",
              "                <td>0</td>\n",
              "                <td class=\"right\" data-ratio=\"6 6\">100%</td>\n",
              "            </tr>\n",
              "            <tr class=\"region\">\n",
              "                <td class=\"name left\"><a href=\"z_36f028580bb02cc8_test_write_to_file_py.html\">test/test_write_to_file.py</a></td>\n",
              "                <td>8</td>\n",
              "                <td>0</td>\n",
              "                <td>0</td>\n",
              "                <td class=\"right\" data-ratio=\"8 8\">100%</td>\n",
              "            </tr>\n",
              "        </tbody>\n",
              "        <tfoot>\n",
              "            <tr class=\"total\">\n",
              "                <td class=\"name left\">Total</td>\n",
              "                <td>77</td>\n",
              "                <td>2</td>\n",
              "                <td>0</td>\n",
              "                <td class=\"right\" data-ratio=\"75 77\">97%</td>\n",
              "            </tr>\n",
              "        </tfoot>\n",
              "    </table>\n",
              "    <p id=\"no_rows\">\n",
              "        No items found using the specified filter.\n",
              "    </p>\n",
              "</main>\n",
              "<footer>\n",
              "    <div class=\"content\">\n",
              "        <p>\n",
              "            <a class=\"nav\" href=\"https://coverage.readthedocs.io/en/7.5.3\">coverage.py v7.5.3</a>,\n",
              "            created at 2024-06-06 15:50 +0000\n",
              "        </p>\n",
              "    </div>\n",
              "    <aside class=\"hidden\">\n",
              "        <a id=\"prevFileLink\" class=\"nav\" href=\"z_36f028580bb02cc8_test_write_to_file_py.html\"></a>\n",
              "        <a id=\"nextFileLink\" class=\"nav\" href=\"z_d0c694b4a484f635___init___py.html\"></a>\n",
              "        <button type=\"button\" class=\"button_prev_file\" data-shortcut=\"[\"></button>\n",
              "        <button type=\"button\" class=\"button_next_file\" data-shortcut=\"]\"></button>\n",
              "        <button type=\"button\" class=\"button_show_hide_help\" data-shortcut=\"?\"></button>\n",
              "    </aside>\n",
              "</footer>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A real example from [FastAPI](https://github.com/tiangolo/fastapi), just check on their impressive 100% code coverage badge to see their report!"
      ],
      "metadata": {
        "id": "SvABrRbuWKar"
      },
      "id": "SvABrRbuWKar"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}